{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing libraries: \n",
    "# pip install geopy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# removing duplciate lane entries\n",
    "# removing lane info \"_lane\" from dataset\n",
    "import pandas as pd\n",
    "\n",
    "def clean_dataset(df):\n",
    "    # Group the data by the two columns and keep only the first occurrence of each group\n",
    "    reduced_df = df.drop_duplicates(subset=[5, 6], keep='first')\n",
    "    # Convert column 1 values to strings and remove anything after and including the last underscore\n",
    "    #reduced_df[1] = reduced_df[1].astype(str).str.rsplit('_', n=1).str[0]\n",
    "    reduced_df.loc[:, 1] = reduced_df.loc[:, 1].astype(str).str.rsplit('_', n=1).str[0]\n",
    "\n",
    "    reduced_df.iloc[:, 5] = reduced_df.iloc[:, 5].astype(str).apply(lambda x: float(x[:2] + '.' + x[2:]))\n",
    "    reduced_df.iloc[:, 6] = reduced_df.iloc[:, 6].astype(str).apply(lambda x: float(x[:4] + '.' + x[4:]))\n",
    "\n",
    "    return reduced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# group data based on sensor location description \n",
    "import pandas as pd\n",
    "\n",
    "def get_sensor_group(data, sensor_loc, ignore=None):\n",
    "    # Filter the DataFrame based on column 3 and ignore string\n",
    "    mask = data[3].str.startswith(sensor_loc) & (~data[3].str.contains(ignore) if ignore is not None else True)\n",
    "    matching_group = data[mask]\n",
    "    return matching_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting the sensor groups based on travelling-salesman-problem algorithm:\n",
    "# Its working as intended\n",
    "# Make it a function to take sensor group and to produce a variable sensor adjacency list as well\n",
    "\n",
    "import pandas as pd\n",
    "import folium\n",
    "from geopy.distance import distance\n",
    "\n",
    "def calculate_distance(sensor_data):\n",
    "    # Calculate distances between each pair of sensors\n",
    "    num_sensors = len(sensor_data)\n",
    "    distances = [[0.0] * num_sensors for _ in range(num_sensors)]\n",
    "    for i in range(num_sensors):\n",
    "        for j in range(i+1, num_sensors):\n",
    "            coord1 = (sensor_data.iloc[i][5], sensor_data.iloc[i][6])\n",
    "            coord2 = (sensor_data.iloc[j][5], sensor_data.iloc[j][6])\n",
    "            dist = distance(coord1, coord2).meters\n",
    "            distances[i][j] = dist\n",
    "            distances[j][i] = dist\n",
    "    return distances\n",
    "\n",
    "def tsp(sensor_data):\n",
    "    \n",
    "    distances = calculate_distance(sensor_data)\n",
    "    map = folium.Map(location=[sensor_data[5].mean(), sensor_data[6].mean()], zoom_start=10)\n",
    "\n",
    "    # Connect sensors with the shortest distance\n",
    "    num_sensors = len(sensor_data)\n",
    "    connected = set()\n",
    "    min_distance = float('inf')\n",
    "    min_i, min_j = -1, -1\n",
    "\n",
    "    # Find the pair of sensors with the shortest distance\n",
    "    for i in range(num_sensors):\n",
    "        for j in range(i+1, num_sensors):\n",
    "            if distances[i][j] < min_distance:\n",
    "                min_distance = distances[i][j]\n",
    "                min_i, min_j = i, j\n",
    "\n",
    "    # Connect the pair of sensors with a polyline\n",
    "    if min_i != -1 and min_j != -1:\n",
    "        sensor1 = sensor_data.iloc[min_i]\n",
    "        sensor2 = sensor_data.iloc[min_j]\n",
    "        folium.PolyLine([(sensor1[5], sensor1[6]), (sensor2[5], sensor2[6])],\n",
    "                        color='red').add_to(map)\n",
    "        connected.add(min_i)\n",
    "        connected.add(min_j)\n",
    "\n",
    "    # Connect the remaining sensors\n",
    "    while len(connected) < num_sensors:\n",
    "        min_distance = float('inf')\n",
    "        min_i = -1\n",
    "        for i in range(num_sensors):\n",
    "            if i not in connected:\n",
    "                for j in connected:\n",
    "                    if distances[i][j] < min_distance:\n",
    "                        min_distance = distances[i][j]\n",
    "                        min_i = i\n",
    "                        min_j = j\n",
    "        if min_i != -1:\n",
    "            sensor1 = sensor_data.iloc[min_i]\n",
    "            sensor2 = sensor_data.iloc[min_j]\n",
    "            folium.PolyLine([(sensor1[5], sensor1[6]), (sensor2[5], sensor2[6])],\n",
    "                            color='red').add_to(map)\n",
    "            connected.add(min_i)\n",
    "\n",
    "    \n",
    "    for _, sensor in sensor_data.iterrows():\n",
    "        tooltip_text = f\"Sensor_ID: {sensor[1]}, Des: {sensor[3]}\"\n",
    "        folium.CircleMarker(\n",
    "            location=[sensor[5], sensor[6]],\n",
    "            radius=5,\n",
    "            color='blue',\n",
    "            fill=True,\n",
    "            fill_color='blue',\n",
    "            tooltip=tooltip_text  # Add tooltip\n",
    "            ).add_to(map)\n",
    "    return map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print adj list using the list of sensors: \n",
    "\n",
    "import pandas as pd\n",
    "import folium\n",
    "from geopy.distance import distance\n",
    "\n",
    "def calculate_distance(sensor_data):\n",
    "    # Calculate distances between each pair of sensors\n",
    "    num_sensors = len(sensor_data)\n",
    "    distances = [[0.0] * num_sensors for _ in range(num_sensors)]\n",
    "    for i in range(num_sensors):\n",
    "        for j in range(i+1, num_sensors):\n",
    "            coord1 = (sensor_data.iloc[i][5], sensor_data.iloc[i][6])\n",
    "            coord2 = (sensor_data.iloc[j][5], sensor_data.iloc[j][6])\n",
    "            dist = distance(coord1, coord2).meters\n",
    "            distances[i][j] = dist\n",
    "            distances[j][i] = dist\n",
    "    return distances\n",
    "\n",
    "def tsp(sensor_data):\n",
    "    distances = calculate_distance(sensor_data)\n",
    "    \n",
    "    # Create an empty list to store the adjacency list\n",
    "    adjacency_list = []\n",
    "\n",
    "    # Connect sensors with the shortest distance\n",
    "    num_sensors = len(sensor_data)\n",
    "    connected = set()\n",
    "    min_distance = float('inf')\n",
    "    min_i, min_j = -1, -1\n",
    "\n",
    "    # Find the pair of sensors with the shortest distance\n",
    "    for i in range(num_sensors):\n",
    "        for j in range(i+1, num_sensors):\n",
    "            if distances[i][j] < min_distance:\n",
    "                min_distance = distances[i][j]\n",
    "                min_i, min_j = i, j\n",
    "\n",
    "    # Connect the pair of sensors\n",
    "    if min_i != -1 and min_j != -1:\n",
    "        adjacency_list.append([sensor_data.iloc[min_i][1], sensor_data.iloc[min_j][1], min_distance])\n",
    "        connected.add(min_i)\n",
    "        connected.add(min_j)\n",
    "\n",
    "    # Connect the remaining sensors\n",
    "    while len(connected) < num_sensors:\n",
    "        min_distance = float('inf')\n",
    "        min_i = -1\n",
    "        for i in range(num_sensors):\n",
    "            if i not in connected:\n",
    "                for j in connected:\n",
    "                    if distances[i][j] < min_distance:\n",
    "                        min_distance = distances[i][j]\n",
    "                        min_i = i\n",
    "                        min_j = j\n",
    "        if min_i != -1:\n",
    "            adjacency_list.append([sensor_data.iloc[min_i][1], sensor_data.iloc[min_j][1], min_distance])\n",
    "            connected.add(min_i)\n",
    "\n",
    "    # Create a DataFrame from the adjacency list\n",
    "    adjacency_df = pd.DataFrame(adjacency_list, columns=['sensor1', 'sensor2', 'cost'])\n",
    "    return adjacency_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_21296\\1195696214.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reduced_df.loc[:, 1] = reduced_df.loc[:, 1].astype(str).str.rsplit('_', n=1).str[0]\n",
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_21296\\1195696214.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reduced_df.iloc[:, 5] = reduced_df.iloc[:, 5].astype(str).apply(lambda x: float(x[:2] + '.' + x[2:]))\n",
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_21296\\1195696214.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reduced_df.iloc[:, 6] = reduced_df.iloc[:, 6].astype(str).apply(lambda x: float(x[:4] + '.' + x[4:]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      sensor1    sensor2        cost\n",
      "0   421_3_507  452_1_508  398.821527\n",
      "1   421_2_506  421_3_507  521.119646\n",
      "2   421_1_505  421_2_506  543.986851\n",
      "3   420_3_160  421_1_505  529.107786\n",
      "4   420_2_160  420_3_160  519.764609\n",
      "5   420_1_231  420_2_160  559.560881\n",
      "6   419_3_217  420_1_231  575.378420\n",
      "7   419_2_217  419_3_217  515.204372\n",
      "8   419_1_234  419_2_217  537.507121\n",
      "9   418_3_234  419_1_234  511.763187\n",
      "10  418_2_225  418_3_234  490.235917\n",
      "11  418_1_225  418_2_225  565.468593\n",
      "12  417_3_230  418_1_225  563.803664\n",
      "13  417_2_230  417_3_230  539.065230\n",
      "14  417_1_216  417_2_230  541.163579\n",
      "15  416_3_216  417_1_216  491.605855\n",
      "16  416_2_224  416_3_216  508.964586\n",
      "17  416_1_215  416_2_224  581.555789\n",
      "18  415_3_215  416_1_215  582.886828\n",
      "19  415_2_214  415_3_215  582.886316\n",
      "20  415_1_223  415_2_214  522.311204\n",
      "21  414_3_223  415_1_223  417.316073\n",
      "22  414_2_209  414_3_223  418.640613\n",
      "23  414_1_209  414_2_209  481.385717\n",
      "24  413_3_204  414_1_209  522.964043\n",
      "25  413_2_208  413_3_204  534.029685\n",
      "26  413_1_208  413_2_208  529.171598\n",
      "27  453_1_509  452_1_508  663.231182\n",
      "28  453_2_510  453_1_509  588.398931\n",
      "29  453_3_511  453_2_510  506.659745\n",
      "30  454_1_512  453_3_511  517.235698\n",
      "31  454_2_513  454_1_512  468.719895\n",
      "32  455_1_514  454_2_513  440.333602\n"
     ]
    }
   ],
   "source": [
    "# Run the full process\n",
    "\n",
    "# def main():\n",
    "\n",
    "# Read from the original data and claen it:\n",
    "data = clean_dataset(pd.read_csv('../detector/detectors2018.csv', header=None))\n",
    "\n",
    "# Select sensor group:\n",
    "\n",
    "#sensor_loc = \"CC-215 WB\"\n",
    "#sensor_loc=\"CC-215 EB\"\n",
    "#sensor_loc=\"I-15 NB\"\n",
    "#sensor_loc= \"I-515 SB\"\n",
    "#sensor_loc=\"US-95 SB\"\n",
    "sensor_loc=\"US-95 NB\"\n",
    "\n",
    "\n",
    "# Ignore \"-\" from the sensor group\n",
    "ignore = 'Ramp'\n",
    "\n",
    "# Get list of sensors using the sensor group\n",
    "sensor_group = get_sensor_group(data, sensor_loc, ignore)\n",
    "\n",
    "# Plot the map of the sensor group \n",
    "\n",
    "#sensor_map = tsp(sensor_group)\n",
    "#sensor_map\n",
    "\n",
    "\n",
    "# Call the tsp function to get the sensor adjacency list\n",
    "adjacency_list = tsp(sensor_group)\n",
    "\n",
    "# Print the adjacency list\n",
    "print(adjacency_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation based on sensor-group:\n",
    "# Scripts takes in the sensor list from sensor group and merges the data files to a dataset\n",
    "\n",
    "# Matching sensor data with sensor files downloaded:\n",
    "\n",
    "# Read from the original data and claen it:\n",
    "data = clean_dataset(pd.read_csv('../detector/detectors2018.csv', header=None))\n",
    "# Get the list of sensors using the sensor group name:\n",
    "sensor_loc=\"I-15 NB\"\n",
    "ignore = 'Ramp'\n",
    "sensor_group = get_sensor_group(data, sensor_loc, ignore)\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "# This script merges individual sensor data files from a set of specific list of sensor name\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# read the list of file names from the csv file\n",
    "#file_list = pd.read_csv('/home/tzahid/Desktop/pred_sensor_list.csv')['pred_sensor'].tolist()\n",
    "file_list=sensor_group[1]\n",
    "\n",
    "# add '.csv' extension to file names in the list\n",
    "file_list = ['d_' + filename + '.csv' for filename in file_list]\n",
    "\n",
    "# get a list of all csv files in the folder\n",
    "path = '/home/tzahid/Desktop/new2018dump/'\n",
    "all_files = os.listdir(path)\n",
    "csv_files = [filename for filename in all_files if filename.endswith('.csv') and filename in file_list]\n",
    "\n",
    "\n",
    "print(f\"{len(csv_files)} CSV files found matching with the list of file names.\")\n",
    "\n",
    "\n",
    "# get the range of index values\n",
    "max_index = 0\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        data = pd.read_csv(os.path.join(path, file), usecols=[0])\n",
    "        max_index = max(max_index, data['index_date'].max())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file}. Skipping to next file.\")\n",
    "\n",
    "index_range = range(1, max_index+1)\n",
    "\n",
    "# create an empty dataframe with the desired index\n",
    "df = pd.DataFrame(index=index_range)\n",
    "\n",
    "# loop through the csv files and extract the 5th column and add it to the dataframe\n",
    "for file in tqdm(csv_files, desc='Processing CSV files'):\n",
    "    try:\n",
    "        col_name = os.path.splitext(file)[0]\n",
    "        col_name = col_name.replace(\" \", \"_\")  # replace spaces with underscores in column names\n",
    "        data = pd.read_csv(os.path.join(path, file), usecols=[0, 4], header=0, skiprows=[0], names=['index_date', col_name]) # modify usecols to define which columns to extract, here 0 and 4\n",
    "        data = data.drop_duplicates(subset=['index_date'])  # drop any duplicate rows\n",
    "        data = data.set_index('index_date')  # set the index to the 'index_date' column\n",
    "        data = data.reindex(index_range, fill_value=0)  # add missing index rows and fill with 0\n",
    "        df[col_name] = data[col_name]  # add the column to the main dataframe\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file}. Skipping to next file.\")\n",
    "\n",
    "# save the concatenated dataframe to a new csv file\n",
    "# name of the final file\n",
    "_filename= sensor_loc +' without '+ ignore +'.csv'\n",
    "df.to_csv(os.path.join(path, _filename), index_label='index_date')\n",
    "\n",
    "# print the range of index values\n",
    "print(f\"Index range: 1 - {max_index}\")\n",
    "\n",
    "# print final message\n",
    "print(f\"Concatenation complete. {len(df.columns)} files have been concatenated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import miceforest as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Analysis\n",
    "\n",
    "raw_data=pd.read_csv(\"E:/I-15 Datataset/2018/I-15 NB without Ramp.csv\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get the row and column number of the dataframe\n",
    "\n",
    "num_rows, num_columns = raw_data.shape\n",
    "print(f\"raw data rows: {num_rows} columns: {num_columns}\")\n",
    "#raw_data.head(10)\n",
    "\n",
    "# Replace 0 by NaN\n",
    "data_masked = raw_data.mask(raw_data == 0, np.nan)\n",
    "data_masked.head(15)\n",
    "\n",
    "# Remove columns which has less than threshold amount of rows\n",
    "threshold_percentage = 80\n",
    "thresh_value = int((threshold_percentage / 100) * len(data_masked))\n",
    "df_cleaned = data_masked.dropna(axis=1, thresh=thresh_value)\n",
    "num_rows, num_columns = df_cleaned.shape\n",
    "print(f\"cleaned data rows: {num_rows} columns: {num_columns}\")\n",
    "\n",
    "# Display the NaN percentage for each column and total\n",
    "nan_percentage = df_cleaned.isna().mean() * 100\n",
    "total_nan_percentage=nan_percentage.mean()\n",
    "\n",
    "print(f\"Column NaN %: \\n{nan_percentage}\")\n",
    "print(f\"Total NaN %: {total_nan_percentage}\")\n",
    "\n",
    "#print(df_cleaned)\n",
    "#df_cleaned.head(30)\n",
    "\n",
    "# Impute the dataframe with NaN values\n",
    "\n",
    "kds = mf.ImputationKernel(\n",
    "  df_cleaned,\n",
    "  datasets=1,\n",
    "  save_all_iterations=True,\n",
    "  random_state=1991\n",
    ")\n",
    "\n",
    "# Run the MICE algorithm for 3 iterations\n",
    "kds.mice(3)\n",
    "\n",
    "print(\"Data imputation complete!\")\n",
    "# Sample into 5/10/15 mins windows evenly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_complete= kds.complete_data()\n",
    "data_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_complete= kds.complete_data()\n",
    "data_complete.iloc[18684,:]\n",
    "\n",
    "data_complete.columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_complete= kds.complete_data()\n",
    "data_complete.head(10)\n",
    "\n",
    "date_index=pd.read_csv(\"E:/I-15 Datataset/2018/dates.csv\")\n",
    "timed_data_complete= pd.merge(data_complete, date_index, left_on='index_date', right_on='date_index', how='inner')\n",
    "# timed_data_complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "timed_data_complete['date_time'] = pd.to_datetime(timed_data_complete['date_time'])\n",
    "timed_data_complete.set_index('date_time', inplace=True)\n",
    "\n",
    "timed_sampled = timed_data_complete.resample('5T').mean()\n",
    "\n",
    "print(timed_sampled)\n",
    "\n",
    "output_file_path = 'E:/I-15 Datataset/2018/5min_sampled_data.csv'  # Replace with the desired file path\n",
    "timed_sampled.to_csv(output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=date_index=pd.read_csv(\"E:/I-15 Datataset/2018/dates.csv\")\n",
    "T = pd.DataFrame(date_index, index=pd.to_datetime(date_index[date_time]))\n",
    "\n",
    "\n",
    "# Check the inferred frequency\n",
    "frequency = pd.infer_freq(df.index)\n",
    "T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
