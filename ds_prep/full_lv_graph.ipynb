{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing libraries: \n",
    "# pip install geopy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# removing duplciate lane entries\n",
    "# removing lane info \"_lane\" from dataset\n",
    "import pandas as pd\n",
    "\n",
    "def clean_dataset(df):\n",
    "    # Group the data by the two columns and keep only the first occurrence of each group\n",
    "    reduced_df = df.drop_duplicates(subset=[5, 6], keep='first')\n",
    "    # Convert column 1 values to strings and remove anything after and including the last underscore\n",
    "    #reduced_df[1] = reduced_df[1].astype(str).str.rsplit('_', n=1).str[0]\n",
    "    reduced_df.loc[:, 1] = reduced_df.loc[:, 1].astype(str).str.rsplit('_', n=1).str[0]\n",
    "\n",
    "    reduced_df.iloc[:, 5] = reduced_df.iloc[:, 5].astype(str).apply(lambda x: float(x[:2] + '.' + x[2:]))\n",
    "    reduced_df.iloc[:, 6] = reduced_df.iloc[:, 6].astype(str).apply(lambda x: float(x[:4] + '.' + x[4:]))\n",
    "\n",
    "    return reduced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# group data based on sensor location description \n",
    "import pandas as pd\n",
    "\n",
    "def get_sensor_group(data, sensor_loc, ignore=None):\n",
    "    # Filter the DataFrame based on column 3 and ignore string\n",
    "    mask = data[3].str.startswith(sensor_loc) & (~data[3].str.contains(ignore) if ignore is not None else True)\n",
    "    matching_group = data[mask]\n",
    "    return matching_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting the sensor groups based on travelling-salesman-problem algorithm:\n",
    "# Its working as intended\n",
    "# Make it a function to take sensor group and to produce a variable sensor adjacency list as well\n",
    "\n",
    "import pandas as pd\n",
    "import folium\n",
    "from geopy.distance import distance\n",
    "\n",
    "def calculate_distance(sensor_data):\n",
    "    # Calculate distances between each pair of sensors\n",
    "    num_sensors = len(sensor_data)\n",
    "    distances = [[0.0] * num_sensors for _ in range(num_sensors)]\n",
    "    for i in range(num_sensors):\n",
    "        for j in range(i+1, num_sensors):\n",
    "            coord1 = (sensor_data.iloc[i][5], sensor_data.iloc[i][6])\n",
    "            coord2 = (sensor_data.iloc[j][5], sensor_data.iloc[j][6])\n",
    "            dist = distance(coord1, coord2).meters\n",
    "            distances[i][j] = dist\n",
    "            distances[j][i] = dist\n",
    "    return distances\n",
    "\n",
    "def tsp(sensor_data):\n",
    "    \n",
    "    distances = calculate_distance(sensor_data)\n",
    "    map = folium.Map(location=[sensor_data[5].mean(), sensor_data[6].mean()], zoom_start=10)\n",
    "\n",
    "    # Connect sensors with the shortest distance\n",
    "    num_sensors = len(sensor_data)\n",
    "    connected = set()\n",
    "    min_distance = float('inf')\n",
    "    min_i, min_j = -1, -1\n",
    "\n",
    "    # Find the pair of sensors with the shortest distance\n",
    "    for i in range(num_sensors):\n",
    "        for j in range(i+1, num_sensors):\n",
    "            if distances[i][j] < min_distance:\n",
    "                min_distance = distances[i][j]\n",
    "                min_i, min_j = i, j\n",
    "\n",
    "    # Connect the pair of sensors with a polyline\n",
    "    if min_i != -1 and min_j != -1:\n",
    "        sensor1 = sensor_data.iloc[min_i]\n",
    "        sensor2 = sensor_data.iloc[min_j]\n",
    "        folium.PolyLine([(sensor1[5], sensor1[6]), (sensor2[5], sensor2[6])],\n",
    "                        color='red').add_to(map)\n",
    "        connected.add(min_i)\n",
    "        connected.add(min_j)\n",
    "\n",
    "    # Connect the remaining sensors\n",
    "    while len(connected) < num_sensors:\n",
    "        min_distance = float('inf')\n",
    "        min_i = -1\n",
    "        for i in range(num_sensors):\n",
    "            if i not in connected:\n",
    "                for j in connected:\n",
    "                    if distances[i][j] < min_distance:\n",
    "                        min_distance = distances[i][j]\n",
    "                        min_i = i\n",
    "                        min_j = j\n",
    "        if min_i != -1:\n",
    "            sensor1 = sensor_data.iloc[min_i]\n",
    "            sensor2 = sensor_data.iloc[min_j]\n",
    "            folium.PolyLine([(sensor1[5], sensor1[6]), (sensor2[5], sensor2[6])],\n",
    "                            color='red').add_to(map)\n",
    "            connected.add(min_i)\n",
    "\n",
    "    \n",
    "    for _, sensor in sensor_data.iterrows():\n",
    "        tooltip_text = f\"Sensor_ID: {sensor[1]}, Des: {sensor[3]}\"\n",
    "        folium.CircleMarker(\n",
    "            location=[sensor[5], sensor[6]],\n",
    "            radius=5,\n",
    "            color='blue',\n",
    "            fill=True,\n",
    "            fill_color='blue',\n",
    "            tooltip=tooltip_text  # Add tooltip\n",
    "            ).add_to(map)\n",
    "    return map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full process\n",
    "\n",
    "# def main():\n",
    "\n",
    "# Read from the original data and claen it:\n",
    "data = clean_dataset(pd.read_csv('../detector/detectors2018.csv', header=None))\n",
    "\n",
    "# Select sensor group:\n",
    "\n",
    "#sensor_loc = \"CC-215 WB\"\n",
    "#sensor_loc=\"CC-215 EB\"\n",
    "#sensor_loc=\"I-15 NB\"\n",
    "#sensor_loc= \"I-515 SB\"\n",
    "#sensor_loc=\"US-95 SB\"\n",
    "sensor_loc=\"US-95 NB\"\n",
    "\n",
    "\n",
    "# Ignore \"-\" from the sensor group\n",
    "ignore = 'Ramp'\n",
    "\n",
    "# Get list of sensors using the sensor group\n",
    "sensor_group = get_sensor_group(data, sensor_loc, ignore)\n",
    "\n",
    "# Plot the map of the sensor group \n",
    "\n",
    "sensor_map = tsp(sensor_group)\n",
    "sensor_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation based on sensor-group:\n",
    "# Scripts takes in the sensor list from sensor group and merges the data files to a dataset\n",
    "\n",
    "# Matching sensor data with sensor files downloaded:\n",
    "\n",
    "# Read from the original data and claen it:\n",
    "data = clean_dataset(pd.read_csv('../detector/detectors2018.csv', header=None))\n",
    "# Get the list of sensors using the sensor group name:\n",
    "sensor_loc=\"I-15 NB\"\n",
    "ignore = 'Ramp'\n",
    "sensor_group = get_sensor_group(data, sensor_loc, ignore)\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "# This script merges individual sensor data files from a set of specific list of sensor name\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# read the list of file names from the csv file\n",
    "#file_list = pd.read_csv('/home/tzahid/Desktop/pred_sensor_list.csv')['pred_sensor'].tolist()\n",
    "file_list=sensor_group[1]\n",
    "\n",
    "# add '.csv' extension to file names in the list\n",
    "file_list = ['d_' + filename + '.csv' for filename in file_list]\n",
    "\n",
    "# get a list of all csv files in the folder\n",
    "path = '/home/tzahid/Desktop/new2018dump/'\n",
    "all_files = os.listdir(path)\n",
    "csv_files = [filename for filename in all_files if filename.endswith('.csv') and filename in file_list]\n",
    "\n",
    "\n",
    "print(f\"{len(csv_files)} CSV files found matching with the list of file names.\")\n",
    "\n",
    "\n",
    "# get the range of index values\n",
    "max_index = 0\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        data = pd.read_csv(os.path.join(path, file), usecols=[0])\n",
    "        max_index = max(max_index, data['index_date'].max())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file}. Skipping to next file.\")\n",
    "\n",
    "index_range = range(1, max_index+1)\n",
    "\n",
    "# create an empty dataframe with the desired index\n",
    "df = pd.DataFrame(index=index_range)\n",
    "\n",
    "# loop through the csv files and extract the 5th column and add it to the dataframe\n",
    "for file in tqdm(csv_files, desc='Processing CSV files'):\n",
    "    try:\n",
    "        col_name = os.path.splitext(file)[0]\n",
    "        col_name = col_name.replace(\" \", \"_\")  # replace spaces with underscores in column names\n",
    "        data = pd.read_csv(os.path.join(path, file), usecols=[0, 4], header=0, skiprows=[0], names=['index_date', col_name]) # modify usecols to define which columns to extract, here 0 and 4\n",
    "        data = data.drop_duplicates(subset=['index_date'])  # drop any duplicate rows\n",
    "        data = data.set_index('index_date')  # set the index to the 'index_date' column\n",
    "        data = data.reindex(index_range, fill_value=0)  # add missing index rows and fill with 0\n",
    "        df[col_name] = data[col_name]  # add the column to the main dataframe\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file}. Skipping to next file.\")\n",
    "\n",
    "# save the concatenated dataframe to a new csv file\n",
    "# name of the final file\n",
    "_filename= sensor_loc +' without '+ ignore +'.csv'\n",
    "df.to_csv(os.path.join(path, _filename), index_label='index_date')\n",
    "\n",
    "# print the range of index values\n",
    "print(f\"Index range: 1 - {max_index}\")\n",
    "\n",
    "# print final message\n",
    "print(f\"Concatenation complete. {len(df.columns)} files have been concatenated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import miceforest as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Analysis\n",
    "\n",
    "#raw_data=pd.read_csv(\"E:/I-15 Datataset/2018/I-15 NB without Ramp.csv\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get the row and column number of the dataframe\n",
    "\n",
    "num_rows, num_columns = raw_data.shape\n",
    "print(f\"raw data rows: {num_rows} columns: {num_columns}\")\n",
    "#raw_data.head(10)\n",
    "\n",
    "# Replace 0 by NaN\n",
    "data_masked = raw_data.mask(raw_data == 0, np.nan)\n",
    "data_masked.head(15)\n",
    "\n",
    "# Remove columns which has less than threshold amount of rows\n",
    "threshold_percentage = 80\n",
    "thresh_value = int((threshold_percentage / 100) * len(data_masked))\n",
    "df_cleaned = data_masked.dropna(axis=1, thresh=thresh_value)\n",
    "num_rows, num_columns = df_cleaned.shape\n",
    "print(f\"cleaned data rows: {num_rows} columns: {num_columns}\")\n",
    "\n",
    "# Display the NaN percentage for each column and total\n",
    "nan_percentage = df_cleaned.isna().mean() * 100\n",
    "total_nan_percentage=nan_percentage.mean()\n",
    "\n",
    "print(f\"Column NaN %: \\n{nan_percentage}\")\n",
    "print(f\"Total NaN %: {total_nan_percentage}\")\n",
    "\n",
    "#print(df_cleaned)\n",
    "#df_cleaned.head(30)\n",
    "\n",
    "# Impute the dataframe with NaN values\n",
    "\n",
    "kds = mf.ImputationKernel(\n",
    "  df_cleaned,\n",
    "  datasets=1,\n",
    "  save_all_iterations=True,\n",
    "  random_state=1991\n",
    ")\n",
    "    # Run the MICE algorithm for 3 iterations\n",
    "kds.mice(3)\n",
    "\n",
    "# Sample into 5/10/15 mins windows evenly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_complete= kds.complete_data()\n",
    "data_complete.head(10)\n",
    "\n",
    "date_index=pd.read_csv(\"E:/I-15 Datataset/2018/dates.csv\")\n",
    "timed_data_complete= pd.merge(data_complete, date_index, left_on='index_date', right_on='date_index', how='inner')\n",
    "# timed_data_complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     index_date  d_398_3_411  d_443_2_343  d_444_3_345  \\\n",
      "date_time                                                                \n",
      "2018-01-02 00:00:00     36572.5   115.500000        82.75        240.0   \n",
      "2018-01-02 00:05:00     36577.0   127.000000        89.20        240.0   \n",
      "2018-01-02 00:10:00     36582.0   157.200000        85.00        240.0   \n",
      "2018-01-02 00:15:00     36587.0   186.400000        94.60        240.0   \n",
      "2018-01-02 00:20:00     36592.0   170.400000        94.80        240.0   \n",
      "...                         ...          ...          ...          ...   \n",
      "2018-12-31 23:35:00    373028.0   202.666667       240.00        240.0   \n",
      "2018-12-31 23:40:00    373031.0   150.666667       240.00        240.0   \n",
      "2018-12-31 23:45:00    373033.5   193.000000       240.00        240.0   \n",
      "2018-12-31 23:50:00    373035.5   147.000000       240.00        240.0   \n",
      "2018-12-31 23:55:00    373037.0   158.000000       240.00        240.0   \n",
      "\n",
      "                     d_439_2_333  d_450_2_359  d_445_3_346  d_399_1_413  \\\n",
      "date_time                                                                 \n",
      "2018-01-02 00:00:00   180.000000   113.500000   117.500000   158.000000   \n",
      "2018-01-02 00:05:00   202.600000   114.800000   116.400000   159.200000   \n",
      "2018-01-02 00:10:00   210.000000   111.200000   118.000000   162.800000   \n",
      "2018-01-02 00:15:00   182.400000   109.400000   121.200000   169.400000   \n",
      "2018-01-02 00:20:00   140.400000   112.400000   119.200000   180.200000   \n",
      "...                          ...          ...          ...          ...   \n",
      "2018-12-31 23:35:00   161.666667   116.666667   115.666667   117.666667   \n",
      "2018-12-31 23:40:00   160.333333   116.000000   111.333333   121.000000   \n",
      "2018-12-31 23:45:00   191.000000   113.500000   112.000000   111.000000   \n",
      "2018-12-31 23:50:00   170.000000   113.000000   121.000000   118.500000   \n",
      "2018-12-31 23:55:00   190.000000   117.000000   120.000000   111.000000   \n",
      "\n",
      "                     d_446_3_349  d_441_3_336  ...  d_443_1_344  d_401_2_437  \\\n",
      "date_time                                      ...                             \n",
      "2018-01-02 00:00:00   122.750000   128.750000  ...   116.000000   223.250000   \n",
      "2018-01-02 00:05:00   104.000000   155.600000  ...   117.200000   213.400000   \n",
      "2018-01-02 00:10:00   111.200000   133.800000  ...   116.000000   227.200000   \n",
      "2018-01-02 00:15:00   122.000000   126.800000  ...   115.200000   227.000000   \n",
      "2018-01-02 00:20:00   124.800000   148.600000  ...   114.800000   212.800000   \n",
      "...                          ...          ...  ...          ...          ...   \n",
      "2018-12-31 23:35:00   159.666667   180.000000  ...   130.000000   206.666667   \n",
      "2018-12-31 23:40:00   146.000000   166.666667  ...   133.666667   175.666667   \n",
      "2018-12-31 23:45:00   120.000000   185.000000  ...   145.500000   186.000000   \n",
      "2018-12-31 23:50:00   138.500000   169.000000  ...   119.000000   121.500000   \n",
      "2018-12-31 23:55:00   116.000000   166.000000  ...   116.000000   135.000000   \n",
      "\n",
      "                     d_355_1_156  d_438_2_330  d_522_2_15  d_402_3_443  \\\n",
      "date_time                                                                \n",
      "2018-01-02 00:00:00   240.000000       165.25   95.250000   240.000000   \n",
      "2018-01-02 00:05:00   240.000000       128.40   97.600000   240.000000   \n",
      "2018-01-02 00:10:00   240.000000       155.40   98.600000   240.000000   \n",
      "2018-01-02 00:15:00   240.000000       153.00   95.800000   240.000000   \n",
      "2018-01-02 00:20:00   240.000000       169.60  101.400000   233.600000   \n",
      "...                          ...          ...         ...          ...   \n",
      "2018-12-31 23:35:00   109.333333       240.00   87.000000   202.333333   \n",
      "2018-12-31 23:40:00   147.000000       240.00   88.666667   193.000000   \n",
      "2018-12-31 23:45:00   142.500000       240.00   89.500000   172.500000   \n",
      "2018-12-31 23:50:00   149.500000       240.00   89.500000   190.500000   \n",
      "2018-12-31 23:55:00   118.000000       240.00   92.000000   177.000000   \n",
      "\n",
      "                     d_437_3_327  d_402_4_444  d_356_2_308  date_index  \n",
      "date_time                                                               \n",
      "2018-01-02 00:00:00        240.0   102.750000   116.750000     36572.5  \n",
      "2018-01-02 00:05:00        240.0   120.800000   108.600000     36577.0  \n",
      "2018-01-02 00:10:00        240.0   148.000000   109.800000     36582.0  \n",
      "2018-01-02 00:15:00        240.0   151.000000   108.200000     36587.0  \n",
      "2018-01-02 00:20:00        240.0   159.400000   109.600000     36592.0  \n",
      "...                          ...          ...          ...         ...  \n",
      "2018-12-31 23:35:00        240.0   195.666667   111.333333    373028.0  \n",
      "2018-12-31 23:40:00        240.0   181.666667   111.666667    373031.0  \n",
      "2018-12-31 23:45:00        240.0   178.500000   133.500000    373033.5  \n",
      "2018-12-31 23:50:00        240.0   199.000000   131.000000    373035.5  \n",
      "2018-12-31 23:55:00        240.0   159.000000   153.000000    373037.0  \n",
      "\n",
      "[104832 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "timed_data_complete['date_time'] = pd.to_datetime(timed_data_complete['date_time'])\n",
    "timed_data_complete.set_index('date_time', inplace=True)\n",
    "\n",
    "timed_sampled = timed_data_complete.resample('5T').mean()\n",
    "\n",
    "print(timed_sampled)\n",
    "\n",
    "output_file_path = 'E:/I-15 Datataset/2018/5min_sampled_data.csv'  # Replace with the desired file path\n",
    "timed_sampled.to_csv(output_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
