{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAS VEGAS I15 DS cleaning script (2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing libraries: \n",
    "# pip install geopy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def delane_ds(df):\n",
    "    # Group the data by the two columns and keep only the first occurrence of each group/ Delane \n",
    "    # 5/6= lat/lon columns\n",
    "    reduced_df = df.drop_duplicates(subset=[5, 6], keep='first')\n",
    "\n",
    "    # Convert column 1 values to strings and remove anything after the first rigt _ and and store the rest\n",
    "    reduced_df.loc[:, 1] = reduced_df.loc[:, 1].astype(str).str.rsplit('_', n=1).str[0]\n",
    "\n",
    "    # Add . after the first two numbers in the lat lon column\n",
    "    reduced_df.iloc[:, 5] = reduced_df.iloc[:, 5].astype(str).apply(lambda x: float(x[:2] + '.' + x[2:]))\n",
    "    reduced_df.iloc[:, 6] = reduced_df.iloc[:, 6].astype(str).apply(lambda x: float(x[:4] + '.' + x[4:]))\n",
    "\n",
    "    return reduced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_sensor_group(data, sensor_loc, ignore=None):\n",
    "    # Filter the DataFrame based on column 3 and ignore \"ignore\" string\n",
    "    mask = data[3].str.startswith(sensor_loc) & (~data[3].str.contains(ignore) if ignore is not None else True)\n",
    "    matching_group = data[mask]\n",
    "    return matching_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data here\n",
    "import pandas as pd\n",
    "\n",
    "# PD warning supression\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "######\n",
    "detector_data = pd.read_csv('../detector/detectors2018.csv', header=None)\n",
    "detector_data=delane_ds(detector_data)\n",
    "#####\n",
    "#sensor_loc = \"CC-215 WB\"\n",
    "#sensor_loc=\"CC-215 EB\"\n",
    "#sensor_loc=\"I-15 NB\"\n",
    "#sensor_loc= \"I-515 SB\"\n",
    "#sensor_loc=\"US-95 SB\"\n",
    "sensor_loc=\"US-95 NB\"\n",
    "ignore = 'Ramp'\n",
    "\n",
    "# Get list of sensors using the sensor group\n",
    "sensor_group = get_sensor_group(detector_data, sensor_loc, ignore)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 filled I-15 NB without Ramp.csv has #rows: 373122 #columns: 54\n",
      "Running Imputation...\n",
      "DataFrame saved as: E:/I-15 Datataset/2018/cleaned/I-15 NB without Ramp _CLN.csv\n",
      "0 filled I-15 SB without Ramp.csv has #rows: 373122 #columns: 55\n",
      "Running Imputation...\n",
      "DataFrame saved as: E:/I-15 Datataset/2018/cleaned/I-15 SB without Ramp _CLN.csv\n",
      "0 filled I-515 NB without Ramp.csv has #rows: 373122 #columns: 38\n",
      "Running Imputation...\n",
      "DataFrame saved as: E:/I-15 Datataset/2018/cleaned/I-515 NB without Ramp _CLN.csv\n",
      "0 filled I-515 SB without Ramp.csv has #rows: 373122 #columns: 37\n",
      "Running Imputation...\n",
      "DataFrame saved as: E:/I-15 Datataset/2018/cleaned/I-515 SB without Ramp _CLN.csv\n",
      "0 filled US-95 NB without Ramp.csv has #rows: 373122 #columns: 32\n",
      "Running Imputation...\n",
      "DataFrame saved as: E:/I-15 Datataset/2018/cleaned/US-95 NB without Ramp _CLN.csv\n",
      "0 filled US-95 SB without Ramp.csv has #rows: 373122 #columns: 34\n",
      "Running Imputation...\n",
      "DataFrame saved as: E:/I-15 Datataset/2018/cleaned/US-95 SB without Ramp _CLN.csv\n",
      "0 filled CC-215 EB without Ramp.csv has #rows: 373122 #columns: 34\n",
      "Running Imputation...\n",
      "DataFrame saved as: E:/I-15 Datataset/2018/cleaned/CC-215 EB without Ramp _CLN.csv\n",
      "0 filled CC-215 WB without Ramp.csv has #rows: 373122 #columns: 41\n",
      "Running Imputation...\n",
      "DataFrame saved as: E:/I-15 Datataset/2018/cleaned/CC-215 WB without Ramp _CLN.csv\n"
     ]
    }
   ],
   "source": [
    "# SENSOR DATA CLEANING\n",
    "\n",
    "import pandas as pd \n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import miceforest as mf\n",
    "\n",
    "# CONFIGURATION:Change the following directories everytime as necessary\n",
    "\n",
    "uncleaned_sensor_groups_list=[\"E:/I-15 Datataset/2018/I-15 NB without Ramp.csv\",\n",
    "                              \"E:/I-15 Datataset/2018/I-15 SB without Ramp.csv\",\n",
    "                              \"E:/I-15 Datataset/2018/I-515 NB without Ramp.csv\",\n",
    "                              \"E:/I-15 Datataset/2018/I-515 SB without Ramp.csv\",\n",
    "                              \"E:/I-15 Datataset/2018/US-95 NB without Ramp.csv\",\n",
    "                              \"E:/I-15 Datataset/2018/US-95 SB without Ramp.csv\",\n",
    "                              \"E:/I-15 Datataset/2018/CC-215 EB without Ramp.csv\",\n",
    "                              \"E:/I-15 Datataset/2018/CC-215 WB without Ramp.csv\"]\n",
    "\n",
    "#uncleaned_sensor_date_time=pd.DataFrame(uncleaned_sensor_groups_list)\n",
    "uncleaned_sensor_date_time=pd.read_csv(\"E:/I-15 Datataset/2018/dates.csv\")\n",
    "\n",
    "# Directory to save the cleaned ds\n",
    "clean_sensor_data_dir= \"E:/I-15 Datataset/2018/cleaned/\"\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "# Copying the uncleaned dataset to a new pd so that both copies remains\n",
    "sensor_date_time=uncleaned_sensor_date_time.copy(deep=True)\n",
    "\n",
    "# Task 1:time and date valued of the Index 18685 is missing, by manual check of the dataset and placed on the first row of the ds\n",
    "# First row is to be removed due to the erronous entry at row 0\n",
    "# There is a large time-gap between corresponding Index values 18684 and 18686\n",
    "# Both index and the date-time columns will be filled \n",
    "\n",
    "# Remove first row\n",
    "sensor_date_time=sensor_date_time.iloc[1:]\n",
    "\n",
    "start_time_str = '18-01-18 09:55'\n",
    "end_time_str = '18-01-18 11:21'\n",
    "\n",
    "start_time = pd.to_datetime(start_time_str)\n",
    "end_time = pd.to_datetime(end_time_str)\n",
    "time_difference_minutes = (end_time - start_time).seconds/ 60\n",
    "\n",
    "temp_date_time=[]\n",
    "temp_index=[]\n",
    "\n",
    "# Probably a better way to loop and make a list\n",
    "# new_date_times = [start_time + pd.Timedelta(minutes=minute) for minute in range(1, int(time_difference_minutes))]\n",
    "\n",
    "for minute in range(1,int((time_difference_minutes))):\n",
    "   # Data-Time column data filling\n",
    "   temp_date_time.append(start_time + pd.Timedelta(minutes=minute))\n",
    "   # Index column data filling\n",
    "   temp_index.append(\"18685\" + \"-\" + str(minute))\n",
    "\n",
    "# Formatting the list to DataFrames\n",
    "temp_date_time=pd.DataFrame({\"date_time\":temp_date_time})\n",
    "temp_index=pd.DataFrame({\"date_index\":temp_index})\n",
    "# Concatenate two temp df side by side, axis=1\n",
    "concatenated_df = pd.concat([temp_index, temp_date_time], axis=1)\n",
    "\n",
    "for files in uncleaned_sensor_groups_list:\n",
    "   # Task 2:Load each sensor-group files in loops, overwrite 18685, and insert the temp_index, fill the rest of the columns with zero\n",
    "   # Load uncleaned sensor files data in loop \n",
    "\n",
    "   # Laod the sensor file in loop\n",
    "   temp_df=pd.read_csv(files)\n",
    "   \n",
    "   # Create a DataFrame with 85 rows and N columns of each sensor files and filled with zeros\n",
    "   zero_df = pd.DataFrame(0, index=range(85), columns=range(len(temp_df.columns)-1))\n",
    "   # Making the temp_date_time the size of the sensor group data \n",
    "   temp_index = pd.concat([temp_index, zero_df], axis=1)\n",
    "   temp_index.columns=temp_df.columns\n",
    "\n",
    "   # Concatenate the new df\n",
    "   temp_df=pd.concat([temp_df[0:18684],temp_index,temp_df[18684:373037]], axis=0)\n",
    "   \n",
    "   # Task 3: Imputate the data frame\n",
    "\n",
    "   # Replace 0 by NaN\n",
    "   data_masked = temp_df.mask(temp_df == 0, np.nan)\n",
    "\n",
    "   # Remove columns which has less than threshold amount of rows\n",
    "   threshold_percentage = 80\n",
    "   thresh_value = int((threshold_percentage / 100) * len(data_masked))\n",
    "   # print(data_masked)\n",
    "   temp_df = data_masked.dropna(axis=1, thresh=thresh_value)\n",
    "   num_rows, num_columns = temp_df.shape\n",
    "   print(f\"0 filled {os.path.basename(files)} has #rows: {num_rows} #columns: {num_columns}\")\n",
    "\n",
    "   # Display the NaN percentage for each column and total\n",
    "   nan_percentage = temp_df.isna().mean() * 100\n",
    "   total_nan_percentage=nan_percentage.mean()\n",
    "   # print(f\"Each sensor NaN percentage:\\n{nan_percentage} \\nTotal NaN percentage: {total_nan_percentage}\")\n",
    "\n",
    "   # Impute NaN values using mice imputation\n",
    "\n",
    "   print(\"Running Imputation...\")\n",
    "\n",
    "   #temp_df['index_date'] = pd.to_datetime(temp_df['index_date'])\n",
    "   temp_df_without_index_date = temp_df.drop(columns=['index_date'])\n",
    " \n",
    "   kds = mf.ImputationKernel(\n",
    "      temp_df_without_index_date,\n",
    "      datasets=1,\n",
    "      save_all_iterations=True,\n",
    "      random_state=1991)\n",
    "   \n",
    "    # Run the MICE algorithm for 3 iterations\n",
    "   kds.mice(3)\n",
    " \n",
    "   imputed_data_without_index_date = kds.complete_data()\n",
    "   cleaned_data = pd.concat([temp_df['index_date'], imputed_data_without_index_date], axis=1)\n",
    "\n",
    "\n",
    "   # Save the cleaned and imputated df as CSV\n",
    "   cleaned_file_name=os.path.basename(files)\n",
    "   cleaned_file_name=clean_sensor_data_dir + cleaned_file_name.split('.')[0] + \" _CLN.\" + cleaned_file_name.split('.')[1]\n",
    "   cleaned_data.to_csv(cleaned_file_name,index=False)\n",
    "\n",
    "   # Set the temp index to one column by removing zero columns \n",
    "   temp_index=temp_index.iloc[:, 0:1]\n",
    "   print(\"DataFrame saved as:\", cleaned_file_name)\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
